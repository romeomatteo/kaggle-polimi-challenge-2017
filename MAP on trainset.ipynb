{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext , SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "from pyspark.mllib.linalg import *\n",
    "from scipy.sparse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"App\")\n",
    "conf.set(\"spark.driver.memory\", '4G')\n",
    "#conf = (conf.setMaster('local[*]')\n",
    "        #.set('spark.driver.memory', '4G')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSet = sc.textFile(\"../train.csv\")\n",
    "trainSet = trainSet.map(lambda l: l.split(','))\n",
    "trainSet = trainSet.filter(lambda line: 'userId' not in line)\n",
    "trainSet = trainSet.map(lambda line: (int(line[0]), int(line[1]), float(line[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevantsPerUser = trainSet.filter(lambda line: line[2] >= 8)\\\n",
    "                            .map(lambda line: (line[0], [line[1]]))\\\n",
    "                            .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantsDict = relevantsPerUser.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ap(RankedList, PositiveItems, at=None):\n",
    "    \"\"\"\n",
    "    Calculates AP@_ \n",
    "    \"\"\"\n",
    "    print(\"Recomended: \", RankedList)\n",
    "    RankedList = RankedList[:at]\n",
    "    is_relevant = np.in1d(RankedList, PositiveItems, assume_unique=True)\n",
    "    print(\"Positives: \", PositiveItems)\n",
    "    print(\"is_relevant: \", is_relevant)   \n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(len(is_relevant)))\n",
    "    print(\"P at k :\" , p_at_k)\n",
    "    map_score = np.sum(p_at_k) / np.min([len(PositiveItems), len(RankedList)])\n",
    "    print(map_score)\n",
    "    assert 0 <= map_score <= 1, map_score\n",
    "    \n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "icm = sc.textFile(\"icm.csv\")\n",
    "icm = icm.map(lambda l: l.split(','))\\\n",
    "            .filter(lambda line: line[0] != 'itemId')\\\n",
    "            .map(lambda x: (int(x[0]), int(x[1]), 1))\n",
    "#FOR THE TOP POP\n",
    "itemSet = trainSet.map(lambda x: (x[1], x[2]))\n",
    "itemsCount = trainSet.map(lambda x: (x[1],1)).reduceByKey(lambda x,y : x + y)\n",
    "itemsCount_dict = itemsCount.collectAsMap()\n",
    "#-----------------------------------------\n",
    "featureFreq = icm.map(lambda x: (x[1],1)).reduceByKey(lambda x, y: x + y)\n",
    "featureFreqDict = featureFreq.collectAsMap()\n",
    "prodCount= icm.map(lambda x: x[0]).distinct().count()\n",
    "featureIdf = featureFreq.map(lambda x: (x[0],np.log10(prodCount/x[1])))\n",
    "featureIdfDict = featureIdf.collectAsMap()\n",
    "\n",
    "targetUsers = sc.textFile(\"target_user.csv\").filter(lambda x: \"userId\" not in x).map(lambda x: int(x))\n",
    "targets=targetUsers.collect()\n",
    "\n",
    "######\n",
    "#TEST COMPUTING THE PREDICTION NORMALIZING BY THE N OF FEATURES AND NOT THE SQRT OF IT\n",
    "norms = icm.map(lambda x: (x[0],1))\\\n",
    "                .reduceByKey(lambda x, y: x+y).mapValues(lambda x: np.sqrt(x))\\\n",
    "                .collectAsMap()\n",
    "\n",
    "normalized = icm.map(lambda x: (x[0], x[1], x[2]/norms[x[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4196"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just consider the row of the users to predict\n",
    "#IF you0re asking, only the idf depends on all the training set, in fact it is computed before reducing the trainset\n",
    "trainSet=trainSet.filter(lambda x: x[0] in targets)\n",
    "data = trainSet.map(lambda x: x[2]).collect()\n",
    "rows = trainSet.map(lambda x: x[0]).collect()\n",
    "cols = trainSet.map(lambda x: x[1]).collect()\n",
    "data.append(0)\n",
    "rows.append(15364)\n",
    "cols.append(37142)\n",
    "userItem=csr_matrix((data,(rows,cols)))\n",
    "data = normalized.map(lambda x: x[2]).collect()\n",
    "rows = normalized.map(lambda x: x[0]).collect()\n",
    "cols = normalized.map(lambda x: x[1]).collect()\n",
    "data.append(0)\n",
    "rows.append(37142)\n",
    "cols.append(80)\n",
    "itemFeature = csc_matrix((data,(rows,cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15365, 19716)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userFeature = userItem.dot(itemFeature)\n",
    "userFeature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19716, 19716)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "for f in featureIdfDict.keys():\n",
    "    data.append(featureIdfDict[f])\n",
    "    cols.append(f)\n",
    "    rows.append(f)\n",
    "featureIdf = csr_matrix((data,(rows,cols)))\n",
    "featureIdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15365, 37143)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userProfile = userFeature.dot(featureIdf)\n",
    "prediction = userProfile.dot(itemFeature.transpose())\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numberOfRecommendations=5\n",
    "#TOP POPULAR\n",
    "cost=8\n",
    "avgRatings=itemSet.reduceByKey(lambda x,y: x+y)\n",
    "avgRatings=avgRatings.map(lambda x: (x[0],x[1]/(itemsCount_dict[x[0]]+cost)))\n",
    "avgRatings.take(5)\n",
    "itemOrderByPop=avgRatings.sortBy(lambda x: x[1], ascending=False)\n",
    "itemPop = np.array(itemOrderByPop.map(lambda x: x[0]).collect())\n",
    "seenItems= trainSet.map(lambda x: (x[0],[x[1]])).reduceByKey(lambda x,y: x + y)\n",
    "seenItemsDict=seenItems.collectAsMap()\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "def recommendTopPop(user_id, removeSeen=True):\n",
    "    seenItems = np.array(seenItemsDict[user_id])\n",
    "    recommendedList = itemPop\n",
    "    if(removeSeen):\n",
    "        unseen_mask = np.in1d(recommendedList, seenItems, invert=True)\n",
    "        recommendedList = recommendedList[unseen_mask]       \n",
    "    return recommendedList[:numberOfRecommendations]\n",
    "\n",
    "\n",
    "def fillWithTopPop(recommended,user):\n",
    "    TopPop=recommendTopPop(user)\n",
    "    for i in range (numberOfRecommendations-len(recommended)):\n",
    "        recommended.append(TopPop[i])\n",
    "    return recommended\n",
    "\n",
    "\n",
    "def getRecommended(user):\n",
    "    recommended = []\n",
    "    itemsPred = prediction.getrow(user).toarray()[0]\n",
    "\n",
    "    for i in range(0,len(itemsPred)):\n",
    "        if(itemsPred[i]!=0):\n",
    "                #if i not in seenItemsDict[user]:\n",
    "                recommended.append((i, itemsPred[i]))\n",
    "    recommended.sort(key = lambda x: -x[1])\n",
    "    recommended=recommended[:numberOfRecommendations]\n",
    "    print(recommended)\n",
    "    recommendedItems = list(map(lambda x: x[0], recommended))\n",
    "    if(len(recommendedItems)<numberOfRecommendations):\n",
    "        recommendedItems=fillWithTopPop(recommendedItems, user)\n",
    "    return recommendedItems    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recomendations = sc.parallelize(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendations = recomendations.map(lambda x: (x, getRecommended(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recomendations.map(lambda x: (x[0], ap(x[1],relevantsDict[x[0]])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35-spark",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
